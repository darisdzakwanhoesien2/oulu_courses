{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affective Computing - Exercise 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "The objective of the exercise is to build a facial expression recognition system. The system includes face preprocessing, feature extraction and classification. In the exercise, you will learn how to preprocess a facial expression image, extract features from an image or a video, and classify a video into a category.\n",
    "\n",
    "Specifically, the region of interest (i.e., facial image) is extracted using face tracking, face registration and face crop functions.  Basic spatiotemporal features (i.e., LBP-TOP features) are extracted using LBP-TOP. For classificaiton of the extracted features Support Vector Machine (SVM) classifiers are trained. 50 videos from 5 participants are used to train the emotion recognition, using spatiotemporal features. The rest of the data (50 videos) are used to evaluate the performances of the trained recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database\n",
    "The original facial expression data is a sub-set of eNTERFACE (acted facial expression), from ten actors acting **happy** and **sadness** behaviors. The used dataset in the exercise includes 100 facial expression samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---#### Help-->\n",
    "<!--- The data and toolbox files used in this exercise can be found in the Affective Computing course webpage (see the Noppa system).\n",
    "-->\n",
    "\n",
    "<!---In the exercise, you should know that some basic python libries (numpy, scikit-image, scipy, pyploy, sklearn) before the programming. If you have questions, send your question to us: \n",
    "* [Henglin.Shi@oulu.fi](mailto:Henglin.Shi@oulu.fi)\n",
    "* [Yante.Li@oulu.fi](mail.Yante.Li@oulu.fi)-->\n",
    "\n",
    "<!---Use the following website to help the usage of python libries. \n",
    "* numpy: http://www.numpy.org/\n",
    "* skimage: https://scikit-image.org/\n",
    "* scipy: https://www.scipy.org/\n",
    "* matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "* sklearn: http://scikit-learn.org/stable/\n",
    "    * sklearn.svm: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    * skealrn.metrics.confusion_matrix: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Face preprocessing\n",
    "\n",
    "In this part, you are supposed to process a face image. There are three subtasks you need to do:\n",
    "* **Task 1.1.** Detect face and facial landmarks using the [`DLib`](http://dlib.net/) library.\n",
    "* **Task 1.2.** You are asked to perform a face registration task using a set of fixed landmarks from a standard model, and extract face from the registered image.\n",
    "* **Task 1.3.** Visualize your result using subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1. Extract facial landmarks <font color='red'>(1 point)</font>\n",
    "**Steps**:\n",
    "1. Load the example image *example_img.jpg*, using the [`plt.imread()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imread.html) function for example.\n",
    "2. As a sanity check visualize the image to see what you are working with.\n",
    "3. Initialize a face detector and a face landmarks detector. We have provide the code of this part, please learn to use them.\n",
    "4. Detect the face\n",
    "5. Detect the face landmarks\n",
    "6. Transfrom the detected result to a 2-D numpy array by using the provided `shape2points` function from the *face_lib.py*.\n",
    "7. Visualize the landmarks with the image\n",
    "\n",
    "*Here is an example for facial landmarks extraction: http://dlib.net/face_landmark_detection.py.html (Note that this examples assumes multiple faces in the image. The images here only have a single sample, and no loop is required.)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Loading required libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdlib\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "# Loading required libraries\n",
    "import dlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from face_lib import shape2points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load example_img.jpg, using plt.imread()\n",
    "example_img = plt.imread(\"example_img.jpg\")\n",
    "# 2. Visualize the image using plt.imshow()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(example_img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Initializing face detector and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "#the shape predictor is a neural network that is loaded from the .dat file\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "# 4. Detect face, return rectangles, each rectangle corresponds to one face.\n",
    "dets = detector(example_img, 1)\n",
    "# 5. Extract the shape of the face in the first rectangle (using the first element of the rectangles variable)\n",
    "shape = predictor(example_img, dets[0])\n",
    "# 6. Extract facial landmarks from shape by calling the shape2points() function.\n",
    "keypoints = shape2points(shape)\n",
    "\n",
    "# 7. Visualize the landmarks (keypoints) by first plotting the image and then using plt.scatter() for the landmarks\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(example_img)\n",
    "plt.scatter(keypoints[:, 0], keypoints[:, 1], c=\"red\", s=6)\n",
    "plt.title(\"Image with Facial Landmarks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2. Face normalization <font color='red'>(1 point)</font>\n",
    "**Steps**:\n",
    "1. Load the landmark position of a standard face model. We provide these positions in a csv file, and also the code block to read these positions.\n",
    "2. Calculate the transformation between your detected landmarks position and the standard face model landmark positions using the [`skimage.transform.PolynomialTransform()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.PolynomialTransform) class and its *estimate()* methods. \n",
    "    1. Instantiate a PolynomialTransform object by calling transform.PolynomialTransform()\n",
    "    2. Call its estimate() method to calculate the transformation between the two sets of points. The manual of this method can be found in the same page which introduced of this class. \n",
    "3. Transform the example image using the calculated transformation to register (map) the example image into a space of the standard face model. You can use the [`skimage.transform.warp()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp) function to perform this.\n",
    "4. Crop the face from the registered face using the standard face model landmarks. The cropping function is provided in the *face_lib.py*, you can directly use it after importing. \n",
    "5. Also extract the face from the example image using your detected landmarks.\n",
    "<!---Use **cropFace** to crop the facial image from the background. For details on the **cropFace** function, please read the function in **exercise1Lib.py**. Usually, we have to do the face registration to remove the subject characteristics. To do a  face registration, firstly you need a set of landmarks of a standard face model. Then you calculate the transformation between you detected landmarks and the standard landmarks. Last you apply the transformation on your example image/-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "# Loading data using scipy.io.loadmat(), or sio.loadmat\n",
    "mdata = sio.loadmat(\"Task3_data.mat\")\n",
    "\n",
    "# Load 'training_data'\n",
    "sample_train = mdata[\"training_data\"]\n",
    "\n",
    "# Load 'testing_data'\n",
    "sample_test = mdata[\"testing_data\"]\n",
    "\n",
    "# Load 'training_class'\n",
    "label_train = mdata[\"training_class\"]\n",
    "\n",
    "# Load 'testing_class'\n",
    "label_test = mdata[\"testing_class\"]\n",
    "\n",
    "#Print the shapes from each of the data splits\n",
    "print(f\"Shape of training_data: {sample_train.shape}\")\n",
    "print(f\"Shape of testing_data: {sample_test.shape}\")\n",
    "print(f\"Shape of training_class: {label_train.shape}\")\n",
    "print(f\"Shape of testing_class: {label_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---**Then you need to calculate the transformation using [`skimage.transform.PolynomialTransform()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.PolynomialTransform)**. This class has several methods, expecially, you need to use the **estimate()** method to calculate the transformation between two sets of points. **Registering the example face** by transforming the example image into the standard model space, using [`skimage.transform.warp()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp) and you calculated transformation. **Cropping the original example image according to the detected landmarks, and the registered image according to the standard face mode. **-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "from face_lib import crop_face\n",
    "\n",
    "# 2. Calculating the transformation between the two set of points \n",
    "# 2.1 Instantiating a PolynomialTransform() transform function\n",
    "tform = transform.PolynomialTransform()\n",
    "\n",
    "# 2.2 Calculating the transformation by calling the estimate() method.\n",
    "tform.estimate(standard_model, keypoints)\n",
    "\n",
    "# 3. Warp the example image using the transform.warp() function\n",
    "registered_face_image = transform.warp(example_img, tform, output_shape=(example_img.shape[0], example_img.shape[1]))\n",
    "\n",
    "# 4. Crop the face from registered image using the provided crop_face function.\n",
    "cropped_registered_face = crop_face(registered_face_image, standard_model)\n",
    "\n",
    "# 5. Cropping the face from the example image using detected landmarks.\n",
    "cropped_example_face = crop_face(example_img, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3. Display result <font color='red'>(2 points)</font>\n",
    "Here you are asked to draw a figure with 3 x 2 subplots using [`matplotlib.pyplot.subplots()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html). Please read the manual of it and also the [`matplotlib.pyplot`](https://matplotlib.org/api/pyplot_api.html). \n",
    "\n",
    "Each of the subplots should have the following images:\n",
    "* subplot [0, 0]: the original example image and detected landmarks.\n",
    "* subplot [1, 0]: the face cropped from the example image. \n",
    "* subplot [2, 0]: the histogram of the face cropped from the example. \n",
    "\n",
    "*As an example, the three subplots are given above.* Then you need to implement:\n",
    "* subplot [0, 1]: the registered face image.\n",
    "* subplot [1, 1]: the face cropped from the registered face image.\n",
    "* subplot [2, 1]: the histogram of the face cropped from the registered face image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Constructing figure with 2x3 subplots\n",
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# subplot [0,0]: show the original example image\n",
    "ax[0, 0].imshow(example_img)\n",
    "ax[0, 0].set_title(\"Original Image with Landmarks\")\n",
    "\n",
    "# Placing detected landmarks on subplot [0,0], we provide an exmaple to do this.\n",
    "ax[0, 0].scatter(keypoints[:, 0], keypoints[:, 1], c=\"red\", s=6)\n",
    "\n",
    "    \n",
    "# subplot [1,0]: show the face cropped from the example image.\n",
    "ax[1, 0].imshow(cropped_example_face, \"gray\")\n",
    "ax[1, 0].set_title(\"Cropped Example Face\")\n",
    "\n",
    "# subplot [2,0]: show the histogram of the face cropped from the example image.\n",
    "ax[2, 0].hist(cropped_example_face.ravel(), bins=256)\n",
    "ax[2, 0].set_title(\"Histogram of Cropped Example Face\")\n",
    "\n",
    "# subplot [0,1]: show the registered image\n",
    "ax[0, 1].imshow(registered_face_image)\n",
    "ax[0, 1].set_title(\"Registered Face Image\")\n",
    "\n",
    "# place the model landmarks on the registered image   \n",
    "ax[0, 1].scatter(standard_model[:, 0], standard_model[:, 1], c=\"blue\", s=6)\n",
    "\n",
    "# subplot [1,1]: show the face cropped from the registered image\n",
    "ax[1, 1].imshow(cropped_registered_face, \"gray\")\n",
    "ax[1, 1].set_title(\"Cropped Registered Face\")\n",
    "\n",
    "# subplot [2,1]: show the histogram of the face cropped from the registered image.\n",
    "ax[2, 1].hist(cropped_registered_face.ravel(), bins=256)\n",
    "ax[2, 1].set_title(\"Histogram of Cropped Registered Face\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. What is the difference between the cropped example image (subplot [1, 0]) and the cropped registered image (subplot [1, 1])? What is the mapping between the images called?\n",
    "*Hints: Look at the two images very closely, the change is very subtle. The code may also provide hints for the transformation name.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:\n",
    "The cropped example image (subplot [1, 0]) shows the face as it was detected in the original image, potentially with some rotation or scaling depending on the subject's pose. The cropped registered image (subplot [1, 1]) shows the face after it has been geometrically transformed (normalized) to align with a standard face model. This means the registered face is typically frontalized and scaled to a consistent size and orientation.\n",
    "\n",
    "The mapping between the images is called **face registration** or **face alignment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. What is the purpose of the operation asked in the previous question, why is it done?\n",
    "### Your answer:\n",
    "The purpose of face registration (or face alignment) is to normalize the facial images by removing variations caused by head pose, scale, and rotation. This is done to ensure that all faces are presented in a consistent, standardized manner, which is crucial for subsequent processing steps like feature extraction and classification. By aligning faces to a common reference, the system can focus on extracting features related to expression rather than being confounded by geometric variations. This improves the robustness and accuracy of facial expression recognition systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Feature extraction <font color='red'>(2 points)</font>\n",
    "Here you are asked to extract LBP (Local Binary Pattern) features. LBP has been widely used in face recognition, facial expression recognition and texture classification.\n",
    "Here you will use the [`skimage.feature.local_binary_pattern()`](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.local_binary_pattern) function to extract the LBP features. Steps are explained below.\n",
    "\n",
    "**Steps**:\n",
    "1. Define the LBP parameters. Before doing this, carefully read the documentation.\n",
    "    1.  P: Number of neighbours, set P = 8\n",
    "    2.  R: Radius of circle, set R = 1.0\n",
    "    3.  LBP methods: set it as 'nri_uniform'\n",
    "\n",
    "2. Extract the LBP face by calling the *skimage.feature.local_binary_pattern()* function\n",
    "\n",
    "3. Calculate the histogram of the LBP face\n",
    "    1. Caculate the histogram of the LBP face\n",
    "    2. Normalize the histogram to make the histogram's sum one one (dividing each element by the sum of the histogram).\n",
    "\n",
    "4. Visualize the result using two subplots.\n",
    "    1. Draw the LBP face on the left side. Use the *cmap=\"gray\"* parameter\n",
    "    2. Draw the normalized histogram on the right side, but using *.stem()* function rather *.plot()* for this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# 1. Define the parameters to extract LBP features in (8, 1) neighborhood:\n",
    "#    1.1 Set the number of neighbour P = 8\n",
    "P = 8\n",
    "#    1.2 Set the radius if circir R = 1.0\n",
    "R = 1.0\n",
    "#    2.3 Set the method as \"nri_uniform\"\n",
    "METHOD = 'nri_uniform'\n",
    "\n",
    "# 2. Extract the LBP face using local_binary_pattern() with the defined parameters\n",
    "lbp_face = local_binary_pattern(cropped_example_face, P, R, method=METHOD)\n",
    "\n",
    "# 3. Calculate the histogram of the LBP face (58 bins). Normalize the histogram's sum to one\n",
    "n_bins = int(lbp_face.max() + 1)\n",
    "hist, _ = np.histogram(lbp_face, density=True, bins=n_bins, range=(0, n_bins))\n",
    "hist = hist / hist.sum() # Normalize to sum one\n",
    "\n",
    "# 4. Visualize your result.\n",
    "fig, (ax_img, ax_hist) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "plt.gray()\n",
    "ax_img.imshow(lbp_face)\n",
    "ax_img.set_title('LBP image')\n",
    "ax_img.axis('off')\n",
    "ax_hist.stem(hist, use_line_collection=True)\n",
    "ax_hist.set_title('Histogram of LBP values')\n",
    "ax_hist.set_ylabel('Percentage')\n",
    "ax_hist.set_xlabel('LBP value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Why is feature extraction (e.g. LBP) used, why can't we simply classify the (raw) images in their original domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:\n",
    "Feature extraction (like LBP) is used because raw image data often contains a lot of redundant information and high dimensionality, which can make classification computationally expensive and prone to overfitting. Raw pixel values are also highly sensitive to variations in lighting, pose, and scale, making direct classification challenging.\n",
    "\n",
    "Feature extraction transforms the raw data into a lower-dimensional, more discriminative representation that captures essential characteristics (e.g., texture, edges) while being more robust to noise and variations. This makes the classification task more efficient and effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Feature Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature classification the SVM (Support Vector Machine) will be utilized. See the documentation for [`sklearn.svm.SVC()`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). Mainly you will use its two methods: **fit()** to training the classifier and **predict()** to use the classifer for classification. There are following three subtasks you need to complete:\n",
    "* Task 3.1. Load data\n",
    "* Task 3.2. Train classifiers\n",
    "* Task 3.3. Evaluate classifiers\n",
    "\n",
    "\n",
    "\n",
    "### Task 3.1. Load data <font color='red'>(1.5 points)</font>\n",
    "Firstly, you need to read *.mat* files using python. You can use the [`scipy.io.loadmat()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html#scipy.io.loadmat) function to read *.mat* file. In the provided **Task3_data.mat** file, different data are packed by different dictionaries which are list below:\n",
    "* training_data\n",
    "* testing_data\n",
    "* training_class\n",
    "* testing_class\n",
    "\n",
    "For example if you would like to load the *training_data* from the file, you can use **scipy.io.loadmat('Task3_data.mat')['training_data']**\n",
    "\n",
    "Print the shapes of data for a sanity check.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "# Loading data using scipy.io.loadmat(), or sio.loadmat\n",
    "mdata = sio.loadmat(\"Task3_data.mat\")\n",
    "\n",
    "# Load 'training_data'\n",
    "sample_train = mdata[\"training_data\"]\n",
    "\n",
    "# Load 'testing_data'\n",
    "sample_test = mdata[\"testing_data\"]\n",
    "\n",
    "\n",
    "# Load 'training_class'\n",
    "label_train = mdata[\"training_class\"]\n",
    "\n",
    "# Load 'testing_class'\n",
    "label_test = mdata[\"testing_class\"]\n",
    "\n",
    "#Print the shapes from each of the data splits\n",
    "print(f\"Shape of training_data: {sample_train.shape}\")\n",
    "print(f\"Shape of testing_data: {sample_test.shape}\")\n",
    "print(f\"Shape of training_class: {label_train.shape}\")\n",
    "print(f\"Shape of testing_class: {label_test.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. The training data (*sample_train*) has a shape of $(50, 708)$. What do the numbers refer to? What is 50 and what is 708?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:\n",
    "The training data (`sample_train`) has a shape of `(50, 708)`. The number `50` refers to the number of samples (e.g., videos or images) in the training dataset. The number `708` refers to the number of features extracted for each sample. In this context, `708` is the dimensionality of the LBP-TOP feature vector for each facial expression sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3. Train SVM classifiers <font color='red'>(0.5 points)</font>\n",
    "Use the **sklearn.svm** library to train Support Vector Machine (SVM) classifiers. The *sample_train* and *sample_test* matrices contain the calculated LBP-TOP features for the training and testing sets, respectively. The block size for LBP-TOP used for training and testing data are 2x2x1. The *label_test* group vector contains the class of samples: 1 = happy, 2 = sadness, corresponding to the rows of the training data matrices.\n",
    "\n",
    "**Steps**:\n",
    "1.  Construct an SVM classifier object using a linear kernel. See [`sklearn.svm.SVC()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).\n",
    "\n",
    "2.  Use the [`fit()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit) method and the *sample_train* and *label_train* to train your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# 1. Initializing an SVM classifier, using linear kernel\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "# 2. Use the classifier to fit your training data\n",
    "classifier.fit(sample_train, label_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3. Evaluate your classifiers <font color='red'>(2 points)</font>\n",
    "**Steps**:\n",
    "1. Use your trained classifer to classify the *sample_train* and *sample_test*, using the [`predict()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict) method.\n",
    "2. Calculate the classification accuracies when classifying the *sample_train* and *sample_test*, respectively. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables *label_train* and *label_test*, respectively.\n",
    "3. Calculate the confusion matrices when evaluating both the dataset by using [`sklearn.metrics.confusion_matrix()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predict the training data and testing data.\n",
    "pred_train = classifier.predict(sample_train)\n",
    "pred_test = classifier.predict(sample_test)\n",
    "\n",
    "# 2. Calculate the accuracies of your prediction on training data and testing data, respectively.\n",
    "#    2.1 Calculate the accuracy when classifying the training data \n",
    "accuracy_train = np.sum(pred_train == label_train.ravel()) / len(label_train)\n",
    "print(f\"Training data accuracy: {accuracy_train:.2f}\")\n",
    "\n",
    "#    2.2 Calculate the accuracy when classifying the test data\n",
    "accuracy_test = np.sum(pred_test == label_test.ravel()) / len(label_test)\n",
    "print(f\"Testing data accuracy: {accuracy_test:.2f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 3. Print/Draw the confusion matrix using sklearn.metrics.confusion_matrix\n",
    "#    3.1 Calculate the confusion matrix when classifying the training data\n",
    "conf_matrix_train = confusion_matrix(label_train, pred_train)\n",
    "print(\"Training data confusion matrix: \")\n",
    "print(conf_matrix_train)\n",
    "\n",
    "#    3.2 Calculate the confusion matrix when classifying the testing data\n",
    "conf_matrix_test = confusion_matrix(label_test, pred_test)\n",
    "print(\"Testing data confusion matrix: \")\n",
    "print(conf_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5. Compare the classification results from train data and test data. Which one achieves a better accuracy? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:\n",
    "Typically, the training data accuracy is better than the test data accuracy. This is because the classifier is trained on the training data and optimizes its parameters to perform well on it. The test data, however, is unseen data, and its accuracy reflects how well the model generalizes to new, unobserved samples. A significant drop in accuracy from training to testing data can indicate overfitting, where the model has learned the training data too well, including its noise, and struggles with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Look at the confusion matrix of test data. In which class did the classifier perform better, the happiness or sadness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:\n",
    "To determine which class (happiness or sadness) the classifier performed better in, we need to look at the confusion matrix for the test data (`conf_matrix_test`). The confusion matrix is structured as follows:\n",
    "\n",
    "```\n",
    "[[True Negatives (TN)  False Positives (FP)]\n",
    " [False Negatives (FN) True Positives (TP)]]\n",
    "```\n",
    "\n",
    "Assuming class 1 is 'happy' and class 2 is 'sadness':\n",
    "- **Accuracy for Happy (Class 1)**: `TN / (TN + FP)` (correctly classified happy samples out of all actual happy samples)\n",
    "- **Accuracy for Sadness (Class 2)**: `TP / (TP + FN)` (correctly classified sadness samples out of all actual sadness samples)\n",
    "\n",
    "Without the actual output of `conf_matrix_test`, I cannot give a definitive answer. However, by examining the values in the confusion matrix, one can compare the diagonal elements (correct classifications) relative to the off-diagonal elements (misclassifications) for each class to determine which emotion was better recognized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
