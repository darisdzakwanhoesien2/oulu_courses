{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affective Computing - Exercise 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "The objective of the exercise is to build a facial expression recognition system. The system includes face preprocessing, feature extraction and classification. In the exercise, you will learn how to preprocess a facial expression image, extract features from an image or a video, and classify a video into a category.\n",
    "\n",
    "Specifically, the region of interest (i.e., facial image) is extracted using face tracking, face registration and face crop functions.  Basic spatiotemporal features (i.e., LBP-TOP features) are extracted using LBP-TOP. For classificaiton of the extracted features Support Vector Machine (SVM) classifiers are trained. 50 videos from 5 participants are used to train the emotion recognition, using spatiotemporal features. The rest of the data (50 videos) are used to evaluate the performances of the trained recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database\n",
    "The original facial expression data is a sub-set of eNTERFACE (acted facial expression), from ten actors acting **happy** and **sadness** behaviors. The used dataset in the exercise includes 100 facial expression samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---#### Help-->\n",
    "<!--- The data and toolbox files used in this exercise can be found in the Affective Computing course webpage (see the Noppa system).\n",
    "-->\n",
    "\n",
    "<!---In the exercise, you should know that some basic python libries (numpy, scikit-image, scipy, pyploy, sklearn) before the programming. If you have questions, send your question to us: \n",
    "* [Henglin.Shi@oulu.fi](mailto:Henglin.Shi@oulu.fi)\n",
    "* [Yante.Li@oulu.fi](mail.Yante.Li@oulu.fi)-->\n",
    "\n",
    "<!---Use the following website to help the usage of python libries. \n",
    "* numpy: http://www.numpy.org/\n",
    "* skimage: https://scikit-image.org/\n",
    "* scipy: https://www.scipy.org/\n",
    "* matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "* sklearn: http://scikit-learn.org/stable/\n",
    "    * sklearn.svm: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    * skealrn.metrics.confusion_matrix: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Face preprocessing\n",
    "\n",
    "In this part, you are supposed to process a face image. There are three subtasks you need to do:\n",
    "* **Task 1.1.** Detect face and facial landmarks using the [`DLib`](http://dlib.net/) library.\n",
    "* **Task 1.2.** You are asked to perform a face registration task using a set of fixed landmarks from a standard model, and extract face from the registered image.\n",
    "* **Task 1.3.** Visualize your result using subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1. Extract facial landmarks <font color='red'>(1 point)</font>\n",
    "**Steps**:\n",
    "1. Load the example image *example_img.jpg*, using the [`plt.imread()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imread.html) function for example.\n",
    "2. As a sanity check visualize the image to see what you are working with.\n",
    "3. Initialize a face detector and a face landmarks detector. We have provide the code of this part, please learn to use them.\n",
    "4. Detect the face\n",
    "5. Detect the face landmarks\n",
    "6. Transfrom the detected result to a 2-D numpy array by using the provided `shape2points` function from the *face_lib.py*.\n",
    "7. Visualize the landmarks with the image\n",
    "\n",
    "*Here is an example for facial landmarks extraction: http://dlib.net/face_landmark_detection.py.html (Note that this examples assumes multiple faces in the image. The images here only have a single sample, and no loop is required.)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required libraries\n",
    "import dlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from face_lib import shape2points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load example_img.jpg, using plt.imread()\n",
    "\n",
    "\n",
    "# 2. Visualize the image using plt.imshow()\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initializing face detector and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "#the shape predictor is a neural network that is loaded from the .dat file\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# 4. Detect face, return rectangles, each rectangle corresponds to one face.\n",
    "# You need to fill the missing argument of this function\n",
    "dets = detector(..., 1)\n",
    "\n",
    "# 5. Extract the shape of the face in the first rectangle (using the first element of the rectangles variable)\n",
    "\n",
    "\n",
    "# 6. Extract facial landmarks from shape by calling the shape2points() function.\n",
    "\n",
    "\n",
    "# 7. Visualize the landmarks (keypoints) by first plotting the image and then using plt.scatter() for the landmarks\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2. Face normalization <font color='red'>(1 point)</font>\n",
    "**Steps**:\n",
    "1. Load the landmark position of a standard face model. We provide these positions in a csv file, and also the code block to read these positions.\n",
    "2. Calculate the transformation between your detected landmarks position and the standard face model landmark positions using the [`skimage.transform.PolynomialTransform()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.PolynomialTransform) class and its *estimate()* methods. \n",
    "    1. Instantiate a PolynomialTransform object by calling transform.PolynomialTransform()\n",
    "    2. Call its estimate() method to calculate the transformation between the two sets of points. The manual of this method can be found in the same page which introduced of this class. \n",
    "3. Transform the example image using the calculated transformation to register (map) the example image into a space of the standard face model. You can use the [`skimage.transform.warp()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp) function to perform this.\n",
    "4. Crop the face from the registered face using the standard face model landmarks. The cropping function is provided in the *face_lib.py*, you can directly use it after importing. \n",
    "5. Also extract the face from the example image using your detected landmarks.\n",
    "<!---Use **cropFace** to crop the facial image from the background. For details on the **cropFace** function, please read the function in **exercise1Lib.py**. Usually, we have to do the face registration to remove the subject characteristics. To do a  face registration, firstly you need a set of landmarks of a standard face model. Then you calculate the transformation between you detected landmarks and the standard landmarks. Last you apply the transformation on your example image/-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the landmark position of the standard face model from a csv file and store them in standard_model\n",
    "def load_landmarks_from_csv(\n",
    "    file_name: str\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads landmarks from a csv file.\n",
    "    Arguments\n",
    "    file_name : A csv file with landmarks\n",
    "    Returns\n",
    "    numpy array with the landmarks\n",
    "    \"\"\"\n",
    "    standard_model = np.zeros((68, 2))\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            line_split = line.replace(\"\\n\", \"\").split(\",\")\n",
    "            standard_model[i] = [float(value) for value in line_split]\n",
    "    # Multiply with 500 (width) as the landmarks are normalized\n",
    "    standard_model *= 500\n",
    "    return standard_model\n",
    "\n",
    "standard_model = load_landmarks_from_csv(\"mean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---**Then you need to calculate the transformation using [`skimage.transform.PolynomialTransform()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.PolynomialTransform)**. This class has several methods, expecially, you need to use the **estimate()** method to calculate the transformation between two sets of points. **Registering the example face** by transforming the example image into the standard model space, using [`skimage.transform.warp()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp) and you calculated transformation. **Cropping the original example image according to the detected landmarks, and the registered image according to the standard face mode. **-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "from face_lib import crop_face\n",
    "\n",
    "# 2. Calculating the transorfmation between the two set of points \n",
    "# 2.1 Instantiating a PolynomialTransform() transform function\n",
    "\n",
    "\n",
    "# 2.2 Calculating the transformation by calling the estimate() method.\n",
    "#     You do not need to retuern any value after calling this methods,\n",
    "#     because the transformation parameter is store in the object you instantiated after calling this methods.\n",
    "\n",
    "\n",
    "# 3. Warp the example image using the transform.warp() function\n",
    "\n",
    "\n",
    "# 4. Crop the face from registered image using the provided crop_face function.\n",
    "\n",
    "\n",
    "# 5. Croping the face from the example image using detected landmarks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3. Display result <font color='red'>(2 points)</font>\n",
    "Here you are asked to draw a figure with 3 x 2 subplots using [`matplotlib.pyplot.subplots()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html). Please read the manual of it and also the [`matplotlib.pyplot`](https://matplotlib.org/api/pyplot_api.html). \n",
    "\n",
    "Each of the subplots should have the following images:\n",
    "* subplot [0, 0]: the original example image and detected landmarks.\n",
    "* subplot [1, 0]: the face cropped from the example image. \n",
    "* subplot [2, 0]: the histogram of the face cropped from the example. \n",
    "\n",
    "*As an example, the three subplots are given above.* Then you need to implement:\n",
    "* subplot [0, 1]: the registered face image.\n",
    "* subplot [1, 1]: the face cropped from the registered face image.\n",
    "* subplot [2, 1]: the histogram of the face cropped from the registered face image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Constructing figure with 2x3 subplots\n",
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# subplot [0,0]: show the original example image\n",
    "ax[0, 0].imshow(example_img)\n",
    "\n",
    "\n",
    "# Placing detected landmarks on subplot [0,0], we provide an exmaple to do this.\n",
    "ax[0, 0].scatter(keypoints[:, 0], keypoints[:, 1], c=\"red\", s=6)\n",
    "\n",
    "    \n",
    "# subplot [1,0]: show the face cropped from the example image.\n",
    "ax[1, 0].imshow(cropped_example_face, \"gray\")\n",
    "\n",
    "\n",
    "# subplot [2,0]: show the histogram of the face cropped from the example image.\n",
    "ax[2, 0].hist(cropped_example_face.ravel(), bins=256)\n",
    "\n",
    "# subplot [0,1]: show the registered image\n",
    "\n",
    "\n",
    "# place the model landmarks on the registered image   \n",
    "\n",
    "\n",
    "# subplot [1,1]: show the face cropped from the registered image\n",
    "\n",
    "\n",
    "# subplot [2,1]: show the histogram of the face cropped from the registered image.\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. What is the difference between the cropped example image (subplot [1, 0]) and the cropped registered image (subplot [1, 1])? What is the mapping between the images called?\n",
    "*Hints: Look at the two images very closely, the change is very subtle. The code may also provide hints for the transformation name.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. What is the purpose of the operation asked in the previous question, why is it done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Feature extraction <font color='red'>(2 points)</font>\n",
    "Here you are asked to extract LBP (Local Binary Pattern) features. LBP has been widely used in face recognition, facial expression recognition and texture classification.\n",
    "Here you will use the [`skimage.feature.local_binary_pattern()`](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.local_binary_pattern) function to extract the LBP features. Steps are explained below.\n",
    "\n",
    "**Steps**:\n",
    "1. Define the LBP parameters. Before doing this, carefully read the documentation.\n",
    "    1.  P: Number of neighbours, set P = 8\n",
    "    2.  R: Radius of circle, set R = 1.0\n",
    "    3.  LBP methods: set it as 'nri_uniform'\n",
    "\n",
    "2. Extract the LBP face by calling the *skimage.feature.local_binary_pattern()* function\n",
    "\n",
    "3. Calculate the histogram of the LBP face\n",
    "    1. Caculate the histogram of the LBP face\n",
    "    2. Normalize the histogram to make the histogram's sum one one (dividing each element by the sum of the histogram).\n",
    "\n",
    "4. Visualize the result using two subplots.\n",
    "    1. Draw the LBP face on the left side. Use the *cmap=\"gray\"* parameter\n",
    "    2. Draw the normalized histogram on the right side, but using *.stem()* function rather *.plot()* for this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# 1. Define the parameters to extract LBP features in (8, 1) neighborhood:\n",
    "#    1.1 Set the number of neighbour P = 8\n",
    "#    1.2 Set the radius if circir R = 1.0\n",
    "#    2.3 Set the method as \"nri_uniform\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Extract the LBP face using local_binary_pattern() with the defined parameters\n",
    "\n",
    "\n",
    "# 3. Calculate the histogram of the LBP face (58 bins). Normalize the histogram's sum to one\n",
    "\n",
    "\n",
    "\n",
    "# 4. Visualize your result.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Why is feature extraction (e.g. LBP) used, why can't we simply classify the (raw) images in their original domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Feature Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature classification the SVM (Support Vector Machine) will be utilized. See the documentation for [`sklearn.svm.SVC()`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). Mainly you will use its two methods: **fit()** to training the classifier and **predict()** to use the classifer for classification. There are following three subtasks you need to complete:\n",
    "* Task 3.1. Load data\n",
    "* Task 3.2. Train classifiers\n",
    "* Task 3.3. Evaluate classifiers\n",
    "\n",
    "\n",
    "\n",
    "### Task 3.1. Load data <font color='red'>(1.5 points)</font>\n",
    "Firstly, you need to read *.mat* files using python. You can use the [`scipy.io.loadmat()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html#scipy.io.loadmat) function to read *.mat* file. In the provided **Task3_data.mat** file, different data are packed by different dictionaries which are list below:\n",
    "* training_data\n",
    "* testing_data\n",
    "* training_class\n",
    "* testing_class\n",
    "\n",
    "For example if you would like to load the *training_data* from the file, you can use **scipy.io.loadmat('Task3_data.mat')['training_data']**\n",
    "\n",
    "Print the shapes of data for a sanity check.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "# Loading data using scipy.io.loadmat(), or sio.loadmat\n",
    "mdata = sio.loadmat(\"Task3_data.mat\")\n",
    "\n",
    "# Load 'training_data'\n",
    "sample_train = mdata[\"training_data\"]\n",
    "\n",
    "# Load 'testing_data'\n",
    "\n",
    "\n",
    "# Load 'training_class'\n",
    "\n",
    "\n",
    "# Load 'testing_class'\n",
    "\n",
    "\n",
    "#Print the shapes from each of the data splits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. The training data (*sample_train*) has a shape of $(50, 708)$. What do the numbers refer to? What is 50 and what is 708?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3. Train SVM classifiers <font color='red'>(0.5 points)</font>\n",
    "Use the **sklearn.svm** library to train Support Vector Machine (SVM) classifiers. The *sample_train* and *sample_test* matrices contain the calculated LBP-TOP features for the training and testing sets, respectively. The block size for LBP-TOP used for training and testing data are 2x2x1. The *label_test* group vector contains the class of samples: 1 = happy, 2 = sadness, corresponding to the rows of the training data matrices.\n",
    "\n",
    "**Steps**:\n",
    "1.  Construct an SVM classifier object using a linear kernel. See [`sklearn.svm.SVC()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).\n",
    "\n",
    "2.  Use the [`fit()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit) method and the *sample_train* and *label_train* to train your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# 1. Initializing an SVM classifier, using linear kernel\n",
    "\n",
    "\n",
    "# 2. Use the classifier to fit your training data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3. Evaluate your classifiers <font color='red'>(2 points)</font>\n",
    "**Steps**:\n",
    "1. Use your trained classifer to classify the *sample_train* and *sample_test*, using the [`predict()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict) method.\n",
    "2. Calculate the classification accuracies when classifying the *sample_train* and *sample_test*, respectively. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables *label_train* and *label_test*, respectively.\n",
    "3. Calculate the confusion matrices when evaluating both the dataset by using [`sklearn.metrics.confusion_matrix()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predict the training data and testing data.\n",
    "\n",
    "\n",
    "\n",
    "# 2. Calculate the accuracies of your prediction on training data and testing data, respectively.\n",
    "#    2.1 Calculate the accuracy when classifying the training data \n",
    "\n",
    "print(\"Training data accuracy: \")\n",
    "\n",
    "\n",
    "#    2.2 Calculate the accuracy when classifying the test data\n",
    "\n",
    "print(\"Testing data accuracy: \")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 3. Print/Draw the confusion matrix using sklearn.metrics.confusion_matrix\n",
    "#    3.1 Calculate the confusion matrix when classifying the training data\n",
    "print(\"Training data confusion matrix: \")\n",
    "\n",
    "\n",
    "\n",
    "#    3.2 Calculate the confusion matrix when classifying the testing data\n",
    "print(\"Testing data confusion matrix: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5. Compare the classification results from train data and test data. Which one achieves a better accuracy? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Look at the confusion matrix of test data. In which class did the classifier perform better, the happiness or sadness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
